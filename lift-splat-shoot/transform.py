import json
import os
import glob
import math
from collections import defaultdict

def load_json(path):
    with open(path, "r", encoding='utf-8') as f:
        return json.load(f)

def save_json(path, data):
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def euler_to_quaternion(roll, pitch, yaw):
    """å°†æ¬§æ‹‰è§’è½¬æ¢ä¸ºå››å…ƒæ•° (w, x, y, z)"""
    cr = math.cos(roll * 0.5)
    sr = math.sin(roll * 0.5)
    cp = math.cos(pitch * 0.5)
    sp = math.sin(pitch * 0.5)
    cy = math.cos(yaw * 0.5)
    sy = math.sin(yaw * 0.5)

    w = cr * cp * cy + sr * sp * sy
    x = sr * cp * cy - cr * sp * sy
    y = cr * sp * cy + sr * cp * sy
    z = cr * cp * sy - sr * sp * cy

    return [w, x, y, z]

def extract_timestamp_from_filename(filename):
    """ä»Žæ–‡ä»¶åæå–æ—¶é—´æˆ³"""
    try:
        parts = filename.split("__")
        timestamp_part = parts[-1].split(".")[0]
        return int(timestamp_part)
    except:
        return None

class TrulyUniversal_LSS_NuScenesGenerator:
    def __init__(self, base_path, annotation_path, alias_config_path=None):
        """
        çœŸæ­£é€šç”¨çš„LSSå…¼å®¹nuScenesæ•°æ®ç”Ÿæˆå™¨

        Args:
            base_path: æ•°æ®æ ¹ç›®å½•
            annotation_path: æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            alias_config_path: å¯é€‰çš„åˆ«åé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽæ™ºèƒ½ç±»åˆ«æ˜ å°„
        """
        self.base_path = base_path
        self.annotation_path = annotation_path
        self.alias_config_path = alias_config_path

        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        self.ensure_directories()

        # è¯»å–ç±»åˆ«é…ç½®
        self.load_existing_categories()

        # åŠ è½½åˆ«åæ˜ å°„
        self.load_alias_config()

        # è·¯å¾„é…ç½®
        self.lidar_path = os.path.join(base_path, "samples/LIDAR_TOP")
        self.camera_paths = {
            'CAM_FRONT_LEFT': os.path.join(base_path, "samples/CAM_FRONT_LEFT"),
            'CAM_FRONT_RIGHT': os.path.join(base_path, "samples/CAM_FRONT_RIGHT"),
            'CAM_BACK_LEFT': os.path.join(base_path, "samples/CAM_BACK_LEFT"),
            'CAM_BACK_RIGHT': os.path.join(base_path, "samples/CAM_BACK_RIGHT")
        }

        # ä¼ æ„Ÿå™¨é…ç½®
        self.load_sensor_configs()

        print(f"ðŸŽ¯ ä»Žcategory.jsonè¯»å–åˆ°çš„ç±»åˆ«: {list(self.categories_by_name.keys())}")
        print(f"ðŸ“‹ ä¼ æ„Ÿå™¨æ˜ å°„: {self.sensor_map}")

    def ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        dirs_to_create = [
            os.path.join(self.base_path, "v1.0-mini"),
            os.path.join(self.base_path, "maps"),
            os.path.join(self.base_path, "samples/LIDAR_TOP"),
            os.path.join(self.base_path, "samples/CAM_FRONT_LEFT"),
            os.path.join(self.base_path, "samples/CAM_FRONT_RIGHT"),
            os.path.join(self.base_path, "samples/CAM_BACK_LEFT"),
            os.path.join(self.base_path, "samples/CAM_BACK_RIGHT"),
        ]
        
        for dir_path in dirs_to_create:
            os.makedirs(dir_path, exist_ok=True)
        
        print(f"âœ… å·²ç¡®ä¿æ‰€æœ‰å¿…è¦ç›®å½•å­˜åœ¨")

    def load_existing_categories(self):
        category_file = os.path.join(self.base_path, "v1.0-mini/category.json")
        try:
            self.categories = load_json(category_file)
            self.categories_by_name = {cat['name']: cat for cat in self.categories}
            self.categories_by_token = {cat['token']: cat for cat in self.categories}
            self.category_tokens = {cat['name']: cat['token'] for cat in self.categories}
            self.category_counters = {cat['name']: 1 for cat in self.categories}
        except FileNotFoundError:
            self.create_basic_categories()
        except Exception as e:
            print(f"âŒ è¯»å–category.jsonå¤±è´¥: {e}")
            self.create_basic_categories()

    def create_basic_categories(self):
        basic_categories = [
            {"token": "category_car", "name": "vehicle", "description": "æœºåŠ¨è½¦è¾†"},
            {"token": "category_human", "name": "human", "description": "è¡Œäºº"}
        ]
        category_file = os.path.join(self.base_path, "v1.0-mini/category.json")
        save_json(category_file, basic_categories)
        self.categories = basic_categories
        self.categories_by_name = {cat['name']: cat for cat in basic_categories}
        self.categories_by_token = {cat['token']: cat for cat in basic_categories}
        self.category_tokens = {cat['name']: cat['token'] for cat in basic_categories}
        self.category_counters = {cat['name']: 1 for cat in basic_categories}
        print("âœ… åˆ›å»ºäº†åŸºç¡€çš„category.jsonæ–‡ä»¶")

    def load_alias_config(self):
        self.alias_mapping = {}
        if self.alias_config_path and os.path.exists(self.alias_config_path):
            try:
                alias_config = load_json(self.alias_config_path)
                for standard_category, aliases in alias_config.items():
                    if standard_category in self.categories_by_name:
                        self.alias_mapping[standard_category.lower()] = standard_category
                        for alias in aliases:
                            self.alias_mapping[alias.lower()] = standard_category
                print(f"âœ… åŠ è½½åˆ«åé…ç½®ï¼Œæ”¯æŒ {len(self.alias_mapping)} ä¸ªåˆ«åæ˜ å°„")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½åˆ«åé…ç½®å¤±è´¥: {e}")
        else:
            self.create_basic_alias_mapping()

    def create_basic_alias_mapping(self):
        basic_aliases = {
            'vehicle': ['car', 'auto', 'automobile', 'truck', 'bus', 'van', 'taxi'],
            'human': ['person', 'people', 'pedestrian', 'man', 'woman', 'child', 'adult', 'walker'],
            'bicycle': ['bike', 'cycle', 'motorcycle', 'motorbike', 'scooter'],
            'barrier': ['cone', 'fence', 'wall', 'obstacle'],
            'sign': ['traffic_sign', 'road_sign', 'billboard', 'signal']
        }
        self.alias_mapping = {}
        for category_name in self.categories_by_name.keys():
            self.alias_mapping[category_name.lower()] = category_name
            if category_name in basic_aliases:
                for alias in basic_aliases[category_name]:
                    self.alias_mapping[alias.lower()] = category_name

    def load_sensor_configs(self):
        try:
            self.sensors = load_json(os.path.join(self.base_path, "v1.0-mini/sensor.json"))
            self.calibrated_sensors = load_json(os.path.join(self.base_path, "v1.0-mini/calibrated_sensor.json"))
            self.sensor_map = {s['channel']: s['token'] for s in self.sensors}
            self.calib_sensor_map = {}
            for cs in self.calibrated_sensors:
                for s in self.sensors:
                    if s['token'] == cs['sensor_token']:
                        self.calib_sensor_map[s['channel']] = cs['token']
                        break
        except FileNotFoundError:
            print("âš ï¸ ä¼ æ„Ÿå™¨é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºé»˜è®¤é…ç½®")
            self.create_default_sensor_configs()

    def create_default_sensor_configs(self):
        """åˆ›å»ºé»˜è®¤çš„ä¼ æ„Ÿå™¨é…ç½®"""
        # åˆ›å»ºsensor.json
        sensors = [
            {"token": "sensor_cam_front_left", "channel": "CAM_FRONT_LEFT", "modality": "camera"},
            {"token": "sensor_cam_front_right", "channel": "CAM_FRONT_RIGHT", "modality": "camera"},
            {"token": "sensor_cam_back_left", "channel": "CAM_BACK_LEFT", "modality": "camera"},
            {"token": "sensor_cam_back_right", "channel": "CAM_BACK_RIGHT", "modality": "camera"},
            {"token": "sensor_lidar_top", "channel": "LIDAR_TOP", "modality": "lidar"}
        ]
        save_json(os.path.join(self.base_path, "v1.0-mini/sensor.json"), sensors)

        # åˆ›å»ºcalibrated_sensor.json
        calibrated_sensors = []
        for sensor in sensors:
            if sensor['modality'] == 'camera':
                calib = {
                    "token": f"calib_{sensor['token']}",
                    "sensor_token": sensor['token'],
                    "translation": [0.0, 0.0, 1.5],
                    "rotation": [1.0, 0.0, 0.0, 0.0],
                    "camera_intrinsic": [
                        [1600.0, 0.0, 800.0],
                        [0.0, 900.0, 450.0],
                        [0.0, 0.0, 1.0]
                    ]
                }
            else:  # lidar
                calib = {
                    "token": f"calib_{sensor['token']}",
                    "sensor_token": sensor['token'],
                    "translation": [0.0, 0.0, 1.8],
                    "rotation": [1.0, 0.0, 0.0, 0.0],
                    "camera_intrinsic": []
                }
            calibrated_sensors.append(calib)
        
        save_json(os.path.join(self.base_path, "v1.0-mini/calibrated_sensor.json"), calibrated_sensors)
        
        # é‡æ–°åŠ è½½é…ç½®
        self.sensors = sensors
        self.calibrated_sensors = calibrated_sensors
        self.sensor_map = {s['channel']: s['token'] for s in self.sensors}
        self.calib_sensor_map = {}
        for cs in self.calibrated_sensors:
            for s in self.sensors:
                if s['token'] == cs['sensor_token']:
                    self.calib_sensor_map[s['channel']] = cs['token']
                    break
        
        print("âœ… åˆ›å»ºäº†é»˜è®¤çš„ä¼ æ„Ÿå™¨é…ç½®æ–‡ä»¶")

    def create_dummy_map_file(self):
        """åˆ›å»ºè™šæ‹Ÿåœ°å›¾æ–‡ä»¶ä»¥æ»¡è¶³NuScenesåº“çš„è¦æ±‚"""
        maps_dir = os.path.join(self.base_path, "maps")
        os.makedirs(maps_dir, exist_ok=True)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„PNGåœ°å›¾æ–‡ä»¶ï¼ˆ100x100åƒç´ çš„ç©ºç™½å›¾ç‰‡ï¼‰
        map_file_path = os.path.join(maps_dir, "custom_map.png")
        if not os.path.exists(map_file_path):
            try:
                from PIL import Image
                import numpy as np
                # åˆ›å»ºä¸€ä¸ª100x100çš„ç©ºç™½å›¾ç‰‡
                img_array = np.zeros((100, 100, 4), dtype=np.uint8)  # RGBA
                img = Image.fromarray(img_array, 'RGBA')
                img.save(map_file_path)
                print(f"âœ… åˆ›å»ºè™šæ‹Ÿåœ°å›¾æ–‡ä»¶: {map_file_path}")
            except ImportError:
                # å¦‚æžœæ²¡æœ‰PILï¼Œåˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶
                with open(map_file_path, 'wb') as f:
                    # å†™å…¥ä¸€ä¸ªæœ€å°çš„PNGæ–‡ä»¶å¤´
                    png_header = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00d\x00\x00\x00d\x08\x02\x00\x00\x00\xff\x80\xbb\x07'
                    f.write(png_header)
                print(f"âœ… åˆ›å»ºç©ºåœ°å›¾æ–‡ä»¶: {map_file_path}")

    def generate_default_metadata_files(self):
        meta_path = os.path.join(self.base_path, "v1.0-mini")
        os.makedirs(meta_path, exist_ok=True)

        log_file = os.path.join(meta_path, "log.json")
        if not os.path.exists(log_file):
            logs = [{
                "token": "log_custom_001",
                "logfile": "custom_logfile",
                "vehicle": "vehicle_001",
                "date_captured": "2025-01-01",
                "location": "custom-dataset"
            }]
            save_json(log_file, logs)
            print("âœ… ç”Ÿæˆ log.json (é»˜è®¤é…ç½®)")

        vis_file = os.path.join(meta_path, "visibility.json")
        if not os.path.exists(vis_file):
            visibility = [
                {"description": "0-40% visible", "token": "vis_1", "level": "v0-40"},
                {"description": "40-60% visible", "token": "vis_2", "level": "v40-60"},
                {"description": "60-80% visible", "token": "vis_3", "level": "v60-80"},
                {"description": "80-100% visible", "token": "vis_4", "level": "v80-100"}
            ]
            save_json(vis_file, visibility)
            print("âœ… ç”Ÿæˆ visibility.json (é»˜è®¤é…ç½®)")

        attr_file = os.path.join(meta_path, "attribute.json")
        if not os.path.exists(attr_file):
            attributes = [
                {"token": "attr_vehicle_moving", "name": "vehicle.moving", "description": "Vehicle is moving."},
                {"token": "attr_vehicle_stopped", "name": "vehicle.stopped", "description": "Vehicle is stationary."},
                {"token": "attr_pedestrian_standing", "name": "pedestrian.standing", "description": "Human is standing."}
            ]
            save_json(attr_file, attributes)
            print("âœ… ç”Ÿæˆ attribute.json (é»˜è®¤é…ç½®)")

        # ä¿®æ”¹ï¼šä½¿ç”¨PNGæ ¼å¼è€Œä¸æ˜¯JSON
        map_file = os.path.join(meta_path, "map.json")
        if not os.path.exists(map_file):
            self.create_dummy_map_file()  # å…ˆåˆ›å»ºåœ°å›¾æ–‡ä»¶
            maps = [{
                "token": "map_custom_001",
                "log_tokens": ["log_custom_001"],
                "category": "semantic_prior",
                "filename": "maps/custom_map.png"  # ä¿®æ”¹ï¼šä½¿ç”¨PNGæ ¼å¼
            }]
            save_json(map_file, maps)
            print("âœ… ç”Ÿæˆ map.json (é»˜è®¤é…ç½®)")

    def generate_scene_json(self):
        """ç”Ÿæˆscene.jsonæ–‡ä»¶"""
        lidar_count = len(glob.glob(os.path.join(self.lidar_path, "*.pcd.bin")))
        scenes = [{
            "token": "orchard_scene_001",
            "log_token": "log_custom_001",
            "nbr_samples": lidar_count,
            "first_sample_token": "sample_000001",
            "last_sample_token": f"sample_{lidar_count:06d}",
            "name": "custom_scene_001",
            "description": "Generated scene from custom dataset"
        }]
        
        output_path = os.path.join(self.base_path, "v1.0-mini/scene.json")
        save_json(output_path, scenes)
        print(f"âœ… scene.json ç”Ÿæˆå®Œæˆï¼šå…± {len(scenes)} ä¸ªåœºæ™¯")
        return scenes

    def smart_normalize_category(self, raw_category):
        if not raw_category:
            return list(self.categories_by_name.keys())[0] if self.categories_by_name else 'unknown'
        raw_category_lower = raw_category.lower().strip()
        for category_name in self.categories_by_name.keys():
            if raw_category_lower == category_name.lower():
                return category_name
        if raw_category_lower in self.alias_mapping:
            return self.alias_mapping[raw_category_lower]
        for category_name in self.categories_by_name.keys():
            if raw_category_lower in category_name.lower() or category_name.lower() in raw_category_lower:
                return category_name
        return self.handle_unknown_category(raw_category)

    def handle_unknown_category(self, raw_category):
        available_categories = list(self.categories_by_name.keys())
        if 'object' in available_categories:
            return 'object'
        return available_categories[0] if available_categories else 'vehicle'

    def analyze_annotation_categories(self):
        print("\nðŸ” åˆ†æžæ ‡æ³¨æ–‡ä»¶ä¸­çš„ç±»åˆ«åˆ†å¸ƒ...")
        ann_files = glob.glob(os.path.join(self.annotation_path, "*.pcd.pcd.json"))
        if not ann_files:
            print("âš ï¸ æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œå°†è·³è¿‡ç±»åˆ«åˆ†æž")
            return {}, {}
            
        raw_categories = defaultdict(int)
        mapping_preview = {}
        for ann_file in ann_files[:3]:
            try:
                ann_data = load_json(ann_file)
                figures = ann_data.get("figures", [])
                objects = ann_data.get("objects", [])
                for fig in figures:
                    obj_key = fig["objectKey"]
                    obj = next((o for o in objects if o["key"] == obj_key), None)
                    if obj:
                        raw_category = obj.get("classTitle", "")
                        if raw_category:
                            raw_categories[raw_category] += 1
                            if raw_category not in mapping_preview:
                                normalized = self.smart_normalize_category(raw_category)
                                mapping_preview[raw_category] = normalized
            except Exception as e:
                print(f"âš ï¸ åˆ†æžæ–‡ä»¶å¤±è´¥ {ann_file}: {e}")
        print(f"\nðŸ“Š å‘çŽ°çš„åŽŸå§‹ç±»åˆ«åŠå…¶æ˜ å°„é¢„è§ˆ:")
        for raw_cat, count in raw_categories.items():
            mapped_cat = mapping_preview.get(raw_cat, "æœªæ˜ å°„")
            print(f"   - '{raw_cat}' ({count}æ¬¡) â†’ '{mapped_cat}'")
        return raw_categories, mapping_preview

    def generate_sample_json(self):
        lidar_files = sorted(glob.glob(os.path.join(self.lidar_path, "*.pcd.bin")))
        if not lidar_files:
            print("âš ï¸ æœªæ‰¾åˆ°LIDARæ–‡ä»¶ï¼Œå°†åˆ›å»ºç©ºçš„sample.json")
            save_json(os.path.join(self.base_path, "v1.0-mini/sample.json"), [])
            return []
            
        samples = []
        
        for i, lidar_file in enumerate(lidar_files):
            filename = os.path.basename(lidar_file)
            timestamp = extract_timestamp_from_filename(filename)
            if timestamp is None:
                timestamp = 1000000 + i * 100000  # ç”Ÿæˆå‡æ—¶é—´æˆ³
                print(f"è­¦å‘Šï¼šæ— æ³•æå–æ—¶é—´æˆ³: {filename}ï¼Œä½¿ç”¨å‡æ—¶é—´æˆ³: {timestamp}")
            
            sample_token = f"sample_{i+1:06d}"
            
            # ä¿®æ”¹ï¼šæ·»åŠ dataå­—æ®µï¼Œè¿™æ˜¯LSSä»£ç å¿…éœ€çš„
            sample_data = {}
            
            # æ·»åŠ LIDARæ•°æ®
            sample_data['LIDAR_TOP'] = f"data_lidar_top_{sample_token}"
            
            # æ·»åŠ ç›¸æœºæ•°æ®
            for camera_name in self.camera_paths.keys():
                camera_name_lower = camera_name.lower()
                sample_data[camera_name] = f"data_{camera_name_lower}_{sample_token}"
            
            sample = {
                "token": sample_token,
                "timestamp": timestamp,
                "scene_token": "orchard_scene_001",
                "prev": f"sample_{i:06d}" if i > 0 else "",
                "next": f"sample_{i+2:06d}" if i < len(lidar_files)-1 else "",
                "data": sample_data,  # æ–°å¢žï¼šå¿…éœ€çš„dataå­—æ®µ
                "anns": []  # ç¨åŽä¼šå¡«å……æ ‡æ³¨token
            }
            samples.append(sample)
        
        output_path = os.path.join(self.base_path, "v1.0-mini/sample.json")
        save_json(output_path, samples)
        print(f"âœ… sample.json ç”Ÿæˆå®Œæˆï¼šå…± {len(samples)} å¸§")
        return samples

    def find_file_by_timestamp(self, folder_path, target_timestamp, pattern):
        if not os.path.exists(folder_path):
            return None
        files = glob.glob(os.path.join(folder_path, pattern))
        for file_path in files:
            filename = os.path.basename(file_path)
            file_timestamp = extract_timestamp_from_filename(filename)
            if file_timestamp and abs(file_timestamp - target_timestamp) < 100000:
                return filename
        return None

    def generate_sample_data_json(self, samples):
        sample_data_list = []
        for sample in samples:
            sample_token = sample['token']
            timestamp = sample['timestamp']
            sample_num = sample_token.split('_')[-1]
            
            # LIDARæ•°æ®
            lidar_filename = self.find_file_by_timestamp(self.lidar_path, timestamp, "*.pcd.bin")
            if lidar_filename:
                lidar_data = {
                    "token": f"data_lidar_top_{sample_token}",
                    "sample_token": sample_token,
                    "ego_pose_token": sample_token,
                    "calibrated_sensor_token": self.calib_sensor_map['LIDAR_TOP'],
                    "timestamp": timestamp,
                    "fileformat": "pcd",
                    "is_key_frame": True,
                    "height": 0,
                    "width": 0,
                    "filename": f"samples/LIDAR_TOP/{lidar_filename}",
                    "prev": f"data_lidar_top_sample_{int(sample_num)-1:06d}" if int(sample_num) > 1 else "",
                    "next": f"data_lidar_top_sample_{int(sample_num)+1:06d}" if int(sample_num) < len(samples) else ""
                }
                sample_data_list.append(lidar_data)
            
            # ç›¸æœºæ•°æ®
            for camera_name, camera_path in self.camera_paths.items():
                camera_filename = self.find_file_by_timestamp(camera_path, timestamp, "*.jpg")
                if camera_filename:
                    camera_name_lower = camera_name.lower()
                    camera_data = {
                        "token": f"data_{camera_name_lower}_{sample_token}",
                        "sample_token": sample_token,
                        "ego_pose_token": sample_token,
                        "calibrated_sensor_token": self.calib_sensor_map[camera_name],
                        "timestamp": timestamp,
                        "fileformat": "jpg",
                        "is_key_frame": True,
                        "height": 900,
                        "width": 1600,
                        "filename": f"samples/{camera_name}/{camera_filename}",
                        "prev": f"data_{camera_name_lower}_sample_{int(sample_num)-1:06d}" if int(sample_num) > 1 else "",
                        "next": f"data_{camera_name_lower}_sample_{int(sample_num)+1:06d}" if int(sample_num) < len(samples) else ""
                    }
                    sample_data_list.append(camera_data)
        
        output_path = os.path.join(self.base_path, "v1.0-mini/sample_data.json")
        save_json(output_path, sample_data_list)
        print(f"âœ… sample_data.json ç”Ÿæˆå®Œæˆï¼šå…± {len(sample_data_list)} æ¡æ•°æ®")

    def generate_truly_universal_sample_annotation_json(self, samples):
        annotations = []
        instances_info = []
        
        # æ›´æ–°samplesçš„annså­—æ®µ
        for sample in samples:
            sample_token = sample['token']
            timestamp = sample['timestamp']
            sample_ann_tokens = []
            
            ann_files = glob.glob(os.path.join(self.annotation_path, "*.pcd.pcd.json"))
            matching_ann_file = None
            for ann_file in ann_files:
                ann_filename = os.path.basename(ann_file)
                ann_timestamp = extract_timestamp_from_filename(ann_filename)
                if ann_timestamp and abs(ann_timestamp - timestamp) < 100000:
                    matching_ann_file = ann_file
                    break
            
            if not matching_ann_file:
                print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°åŒ¹é…çš„æ ‡æ³¨æ–‡ä»¶: {sample_token}")
                continue
                
            try:
                ann_data = load_json(matching_ann_file)
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–æ ‡æ³¨æ–‡ä»¶ {matching_ann_file}: {e}")
                continue
                
            figures = ann_data.get("figures", [])
            objects = ann_data.get("objects", [])
            for fig in figures:
                obj_key = fig["objectKey"]
                obj = next((o for o in objects if o["key"] == obj_key), None)
                if not obj:
                    continue
                    
                raw_category = obj.get("classTitle", "")
                normalized_category = self.smart_normalize_category(raw_category)
                if normalized_category not in self.categories_by_name:
                    continue
                    
                instance_token = f"{normalized_category}_inst_{self.category_counters[normalized_category]:03d}"
                self.category_counters[normalized_category] += 1
                geom = fig["geometry"]
                rotation_info = geom.get("rotation", {"x": 0.0, "y": 0.0, "z": 0.0})
                quaternion = euler_to_quaternion(rotation_info.get("x", 0.0),
                                                 rotation_info.get("y", 0.0),
                                                 rotation_info.get("z", 0.0))
                ann_token = f"ann_{sample_token}_{instance_token}"
                sample_ann_tokens.append(ann_token)  # æ·»åŠ åˆ°sampleçš„annsåˆ—è¡¨
                
                annotation = {
                    "token": ann_token,
                    "sample_token": sample_token,
                    "instance_token": instance_token,
                    "visibility_token": "vis_4",
                    "attribute_tokens": [],
                    "translation": [
                        geom["position"]["x"],
                        geom["position"]["y"],
                        geom["position"]["z"]
                    ],
                    "size": [
                        geom["dimensions"]["x"],
                        geom["dimensions"]["y"],
                        geom["dimensions"]["z"]
                    ],
                    "rotation": quaternion,
                    "prev": "",
                    "next": "",
                    "num_lidar_pts": 0,
                    "num_radar_pts": 0,
                    "category_name": f"{normalized_category}.default"  # æ–°å¢žï¼šLSSéœ€è¦çš„å­—æ®µ
                }
                annotations.append(annotation)
                instances_info.append({
                    'token': instance_token,
                    'category': normalized_category,
                    'annotation_token': ann_token
                })
            
            sample['anns'] = sample_ann_tokens  # æ›´æ–°sampleçš„annså­—æ®µ
        
        # é‡æ–°ä¿å­˜æ›´æ–°åŽçš„samples
        samples_output_path = os.path.join(self.base_path, "v1.0-mini/sample.json")
        save_json(samples_output_path, samples)
        
        output_path = os.path.join(self.base_path, "v1.0-mini/sample_annotation.json")
        save_json(output_path, annotations)
        print(f"âœ… sample_annotation.json ç”Ÿæˆå®Œæˆï¼šå…± {len(annotations)} æ¡æ ‡æ³¨")
        return annotations, instances_info

    def generate_truly_universal_instance_json(self, instances_info):
        instances = []
        for instance_info in instances_info:
            instance_token = instance_info['token']
            category = instance_info['category']
            annotation_token = instance_info['annotation_token']
            category_token = self.category_tokens[category]
            instance = {
                "token": instance_token,
                "category_token": category_token,
                "nbr_annotations": 1,
                "first_annotation_token": annotation_token,
                "last_annotation_token": annotation_token
            }
            instances.append(instance)
        output_path = os.path.join(self.base_path, "v1.0-mini/instance.json")
        save_json(output_path, instances)
        print(f"âœ… instance.json ç”Ÿæˆå®Œæˆï¼šå…± {len(instances)} ä¸ªå®žä¾‹")

    def generate_simple_ego_pose_json(self, samples):
        ego_poses = []
        for sample in samples:
            sample_token = sample['token']
            timestamp = sample['timestamp']
            ego_pose = {
                "token": sample_token,
                "translation": [0.0, 0.0, 0.0],
                "rotation": [1.0, 0.0, 0.0, 0.0],
                "timestamp": timestamp
            }
            ego_poses.append(ego_pose)
        output_path = os.path.join(self.base_path, "v1.0-mini/ego_pose.json")
        save_json(output_path, ego_poses)
        print(f"âœ… ego_pose.json ç”Ÿæˆå®Œæˆï¼šå…± {len(ego_poses)} ä¸ªä½å§¿")
        return ego_poses

    def validate_lss_compatibility(self):
        required_files = [
            "sample.json", "sample_data.json", "sample_annotation.json",
            "instance.json", "category.json", "calibrated_sensor.json",
            "sensor.json", "scene.json", "ego_pose.json", "map.json"
        ]
        missing_files = []
        for file in required_files:
            file_path = os.path.join(self.base_path, f"v1.0-mini/{file}")
            if not os.path.exists(file_path):
                missing_files.append(file)
        if missing_files:
            print(f"   âŒ ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            return False
        else:
            print("   âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨")
            print("   ðŸŽ‰ æ•°æ®å®Œå…¨å…¼å®¹LSSæ ¼å¼ï¼")
            return True

    def generate_all(self):
        print("ðŸš€ å¼€å§‹ç”ŸæˆçœŸæ­£é€šç”¨LSSå…¼å®¹ç‰ˆnuscenesæ ¼å¼æ•°æ®...")
        self.generate_default_metadata_files()
        self.generate_scene_json()  # æ–°å¢žï¼šç”Ÿæˆscene.json
        self.analyze_annotation_categories()
        samples = self.generate_sample_json()
        self.generate_sample_data_json(samples)
        annotations, instances_info = self.generate_truly_universal_sample_annotation_json(samples)
        self.generate_truly_universal_instance_json(instances_info)
        self.generate_simple_ego_pose_json(samples)
        self.validate_lss_compatibility()
        print("ðŸŽ‰ æ•°æ®ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    base_path = "/home/zxy/lift-splat-shoot/nuScenes-1/mini"
    annotation_path = "/home/zxy/lift-splat-shoot/nuScenes-1/mini/samples/project_3_annotations_2025_07_24_07_40_23_sly point cloud format 1.0/ds0/ann"
    alias_config_path = None
    generator = TrulyUniversal_LSS_NuScenesGenerator(
        base_path=base_path,
        annotation_path=annotation_path,
        alias_config_path=alias_config_path
    )
    generator.generate_all()
